---
title: "main"
output: html_document
date: "Last Updated: `r format(Sys.time(), '%b %d, %Y')` " 
---

```{r Dependency Imports, eval=TRUE, echo=FALSE, message=FALSE, include=FALSE}
library("tidyverse")
library("dplyr")
library("here")
library("readr")
library("ggplot2")
library("lubridate")
library("scales")
library("readxl")
```

```{r Question 1) a.}
# Store path to 'data' folder in a list named 'data_paths'
data_paths <- list.files(path = here("data"), full.names = TRUE)
# Character vector to be used as column vectors
nasa_temp_col_names <- c("date", "temp", "dummy")  # 'dummy' will be dropped
# Column specification list
nasa_temp_col_types <- list("date" = col_date(format = "%Y"),
                            "temp" = col_double(),
                            "dummy" = col_skip())   # drop 'dummy'
# Store data in 'nasa_temp' tibble
nasa_temp <- read_table(file = data_paths[4],   # 4th element in list
                        col_names = nasa_temp_col_names,
                        col_types = nasa_temp_col_types,
                        skip = 5)   # skip first 5 rows as they contain no data
ggplot(data=nasa_temp, aes(x=date, y=temp)) +
  geom_line(color="blue")+
  geom_point(color="blue")+
  labs(title="Global Yearly Land-Ocean Temperature",
       x="Date",
       y="Temperature (\u00B0C)")
```

```{r Question 1) b., message=FALSE}
nasa_ice_col_types = list("date" = col_date(format = "%Y"),
                          "X2" = col_skip(),
                          "X3" = col_skip(),
                          "X4" = col_skip(),
                          "ice" = col_double(),
                          "X6" = col_skip())
nasa_ice_col_names = c("date", "X2", "X3", "X4", "ice", "X6")
nasa_ice <- read_csv2(file=data_paths[2],
                     col_names = nasa_ice_col_names,
                     col_types = nasa_ice_col_types,
                     skip = 1)
ggplot(data=nasa_ice, aes(x=date, y=ice)) +
  geom_line(color="blue")+
  geom_point(color="blue")+
  labs(title="Arctic Sea Ice Minimum",
       x="Year",
       y="Minimum arctic sea ice extent in million square km")
```

``` {r Question 1) c.}
nasa_sea_col_types = list(
  "X1" = col_skip(),
  "X2" = col_skip(),
  "date" = col_double(),
  "X4" = col_skip(),
  "X5" = col_skip(),
  "X6" = col_skip(),
  "X7" = col_skip(),
  "X8" = col_skip(),
  "X9" = col_skip(),
  "X10" = col_skip(),
  "X11" = col_skip(),
  "sea" = col_double()
)
nasa_sea_col_names = c("X1", "X2", "date", "X4", "X5", "X6", "X7", "X8", "X9", "X10", "X11", "sea")
nasa_sea <- read_table(file=data_paths[5],
                     col_names = nasa_sea_col_names,
                     col_types = nasa_sea_col_types,
                     skip = 48) %>% 
  mutate(date = date_decimal(date, tz = "UTC")) %>% 
  mutate(date = as_date(date))
ggplot(data=nasa_sea, aes(x=date, y=sea))+
  geom_line(color="blue")+
  labs(title="Sea level variation",
       x="Year",
       y="Sea level variance compared to reference year 2005 (mm)")
```

``` {r Question 1) d.}
nasa_co2_col_types = list(
  "X1" = col_skip(),
  "X2" = col_skip(),
  "date" = col_double(),
  "co2" = col_double(),
  "X5" = col_skip(),
  "X6" = col_skip(),
  "X7" = col_skip(),
  "X8" = col_skip())
nasa_co2_col_names = c("X1", "X2", "date", "co2", "X5", "X6", "X7", "X8")
  
nasa_co2 <- read_table(file=data_paths[3],
                       col_names = nasa_co2_col_names,
                       col_types = nasa_co2_col_types,
                       comment = "#") %>% 
  mutate(date = date_decimal(date)) %>%
  mutate(date = as_date(date)) %>%    # Set 'date' type as date instead of datetime
  mutate(date = floor_date(date, unit="month")) %>% # Round down to nearest unit
  filter(co2 > 0)
ggplot(data=nasa_co2, aes(x=date, y=co2))+
  geom_line(color="blue")+
  labs(title=expression("CO"[2]*" levels over time"),
       x="Year",
       y=expression("Average global CO"[2]*" level in parts per million (ppm)"))
```

```{r Question 1) e., message=FALSE, include=FALSE}
nasa <- left_join(nasa_temp, nasa_ice) %>% 
  full_join(nasa_sea) %>% 
  full_join(nasa_co2)
```

```{r Question 1) f., warning=FALSE}
ggplot(data=nasa %>% filter(date >= as.Date("1960-01-01") & date < as_date("2021-01-01")),
       aes(x=temp, y=co2))+
  geom_point(aes(color=year(date)))+
  scale_color_gradient(low="blue", high="red")+
  labs(title=expression("Relationship between global average temperature and CO"[2]*" levels"),
       x="Temperature (\u00B0C)",
       y=expression("Average global CO"[2]*" level in parts per million (ppm)"),
       color="Year")
```

``` {r Question 2) a.}
historic_co2_col_types = list(
  "yrbp" = col_double(),
  "co2" = col_double())
historic_co2_col_names = c("yrbp", "co2")
historic_co2 <- read_table(file=data_paths[1],
                           skip=774,
                           col_names = historic_co2_col_names,
                           col_types = historic_co2_col_types) 
```

```{r Question 2) b., message=FALSE, include=FALSE}
historic_co2 %>% mutate(yrbp = yrbp + 13) # change ref. year to 2021 (i.e. add 13)
nasa_co2 <- nasa_co2 %>%
  add_column(yrbp = 2021 - as.double(format(nasa_co2$date, format="%Y")))
combined_co2 <- full_join(nasa_co2, historic_co2) %>%
  select(!date) %>% 
  arrange(., yrbp) %>% 
  relocate(2)
```

``` {r Question 3) c.}
options(scipen = 5)
avg <- filter(combined_co2, yrbp == 0) %>% 
  colMeans(co2) %>% 
  unname()
ggplot(data=combined_co2,
       aes(x=yrbp, y=co2))+
  geom_line(color="black",
            size = 0.9)+
  scale_x_reverse(labels = comma)+
  labs(x="Years before present",
       y="Carbon dioxite [ppm]")+
  theme_classic()+
  geom_curve(
    aes(x=70000, y=395,
        xend = 3000, yend = 415.52),
    arrow = arrow(length = unit(0.03, "npc"),
                  type="open"),
    colour = "#EC7014",
    size = 0.9,
    angle = 90,
    curvature = -0.3)+
  geom_text(aes(x = 150000, y = 390,
                label = sprintf("2021 average:\n %g ppm", avg[2]),
                color="#EC7014"))+
  theme(legend.position = "none",
        axis.title = element_text(size = 17),
        axis.text = element_text(size = 12))
```

``` {r Question 3) a., include=FALSE}
sea_ice_1 <- read_excel(path=data_paths[6],
                        sheet=1) %>% 
  fill(1) %>% 
  gather(key="year", value="extent", 3:46) %>% 
  select(-(3:5)) %>% 
  gather(key="key", value="month", 1) %>% 
  select(-4) %>% 
  gather(key="day", value="day", 1) %>% 
  select(-4) %>% 
  mutate(month = recode(month,
                        January = 1,
                        February = 2,
                        March = 3,
                        April = 4,
                        May = 5,
                        June = 6,
                        July = 7,
                        August = 8,
                        September = 9,
                        October = 10,
                        November = 11,
                        December = 12)) %>% 
  relocate(extent) %>% 
  arrange(month, day) %>% 
  mutate(year = as.integer(year)) %>% 
  mutate(month = as.integer(month)) %>% 
  mutate(day = as.integer(day))
```

``` {r Question 3) b., include=FALSE}
mn_79 <- select(sea_ice_1, 1:3) %>%
  filter(year == 1979) %>% 
  group_by(year, month) %>% 
  summarise(mean = (mean(extent, na.rm=TRUE) / 12))
sea_ice_2 <- select(sea_ice_1, 1:3) %>% 
  group_by(year, month) %>% 
  summarise(proportion_baseline_extent = ((mean(extent, na.rm=TRUE) / 12)) / mn_79$mean[month]) %>% 
  arrange(month)
```

``` {r Question 3) c.}
sea_ice_2 %>% filter(year > 1979 & year < 2021) %>% 
  ggplot()+
  geom_tile(aes(x=year, y=month.name[month], fill = proportion_baseline_extent))+
  scale_fill_distiller(palette="RdPu", name = "Proportion of 1979 extent")+
  scale_y_discrete(limits = month.name, expand = expansion(mult =c(0, 0.1)))+
  scale_x_continuous(expand = c(0, 0))+
  labs(title="Sea ice northern hemisphere",
       x="Year",
       y="Month")+
  theme_classic()+
  theme(axis.title = element_text(size = 13),
        plot.title = element_text(size = 20,
                                  hjust = 0.5),
        axis.line = element_line(size = 1),
        axis.ticks = element_line(size = 1))
```
``` {r delete this when done}
this_path <- list.files(path = here("data_cleaned"), full.names = TRUE)
compare <- read_csv(file=this_path[10])
```



``` {r Question 4) a., warning=FALSE}
stop_search_col_types = list(
  "X1" = col_skip(),
  "year" = col_character(),
  "X3" = col_skip(),
  "ethnicity" = col_character(),
  "X5" = col_skip(),
  "legislation_type" = col_character(),
  "geography" = col_character(),
  "X8" = col_skip(),
  "stops" = col_character(),
  "X10" = col_skip(),
  "population" = col_character(),
  "rate" = col_double(),
  "X13" = col_skip(),
  "X14" = col_skip()
)
stop_search_col_names = c("X1", "year", "X3", "ethnicity", "X5",
                          "legislation_type", "geography", "X8",
                          "stops", "X10", "population","rate",
                          "X13", "X14")

stop_search_1 <- read_csv(file=data_paths[7],
                          col_types=stop_search_col_types,
                          col_names=stop_search_col_names,
                          skip=1) %>%
  filter(ethnicity == "All" | ethnicity == "Asian" | ethnicity == "Black" | ethnicity == "White" | ethnicity == "Other") %>% 
  mutate(stops = as.integer(gsub(",", "", stops))) %>% 
  mutate(population = as.integer(gsub(",", "", population)))
```

``` {r Question 4) b.}

rate_white <- select(stop_search_1, c(1, 3, 4 ,7)) %>%
  # Select columns at positions 1, 3, 4 and 7 from the "stop_search_1" tibble
  filter(stop_search_1$ethnicity == "White") %>%
  # Filter out any observations where "ethnicity" variable is not "White"
  mutate(rate_white = rate)
  # Rename column "rate" to "rate_white" for clarity
# Create a new tibble containing the stop-and-search rate for the "White" ethnicity and other variable used for identification

stop_search_2 <- stop_search_1 %>%
  left_join(rate_white, by = c("year", "legislation_type", "geography")) %>%
  # Return all rows from stop_search_1, and any rows with matching "year", "legislation_type" and "geography" from the "rate_white" tibble
  select(-9) %>%
  # Drop redundant "rate_white" column
  rename(rate_white = rate.y) %>%
  # Rename "rate.y" to "rate_white"
  rename(rate = rate.x) %>%
  # Rename "rate.x" to "rate"
  distinct() %>% 
  # Remove duplicate rows (where all variables match)
  mutate(relative_disparity = rate / rate_white)
  # Create a new column and store the result of the rate / rate_white calculation

```

``` {r Question 4) c / d } ```

1) Has the relative disparity of stop-and-search rates for people of Asian and Black ethnicity increased or decreased over the years?

``` {r Q1}

stop_search_2 %>%
  filter(ethnicity != "Other" & ethnicity != "All" & ethnicity != "White") %>%
  # Filter out any ethnicity groups belonging to "Other", "All" and "White"
  ggplot(aes(fill=ethnicity, x=year, y=relative_disparity))+
    geom_col(position = "dodge")+
    # Plot a grouped bar chart ("dodge" to unstack the bars)
    labs(title="Stop-and-search relative disparity over time by ethnicity",
         x="Year",
         y="Relative disparity",
         fill="Ethnicity")+
    # Change title, axis labels and legend label
    scale_y_continuous(expand = c(0,0),
                       # Remove gap between x-axis and bars
                       minor_breaks = seq(0, 100, 10))+
                       # Insert horizontal grid lines between y-values 0-100 in increments of 10
    # Set y-axis as continuous
    theme(axis.title = element_text(size = 13),
          # Resize axis titles
          axis.text.x = element_text(angle=75, vjust = 0.5, face = "bold"),
          # Change angle of x-axis tick labels to 75 deg, move downwards and make font bold
          plot.title = element_text(size = 18, hjust = 0.1),
          # Resize title and move to the right
          panel.grid.major.y = element_line(color = "grey"),
          # Set colour of horizontal grid lines to grey
          panel.grid.major.x = element_blank(),
          # Remove vertical grid lines
          panel.background = element_blank(),
          # Remove background panel
          axis.line = element_line(colour = "black"))
          # Set axis lines colour as black
    # Change theme settings
  # Plot bar chart

```
Between 2006 and 2020, the relative disparity has increased for both ethnic groups, meaning that there is a larger stop-and-search rate for people belonging to these
categories in proportion to people belonging to the White ethnic group than in the past. As observed on the bar plote above, this trend peaked in 2017/18 for the Black ethnic category, and in 2010/11
for the Asian ethnic category.



2)In what geographical area of UK do the most stop-and-searches occur?

``` {r Q2}

stop_search_2 %>%
  filter(geography != "All - excluding BTP" & 
         geography != "All - excluding BTP and Greater Manchester" &
         geography != "All - including BTP and excluding Greater Manchester" &
         geography != "All - including BTP") %>%
  # Filter out non-individual geography groups
  filter(ethnicity != "All") %>%
  # Filter out ethnicity group "All"
  drop_na(stops) %>% 
  # Remove rows where "stops" value is NA
  group_by(geography) %>%
  summarise(stops = sum(stops)) %>%
  # Reduce the number of rows by summing the amount of stops per geographic group
  ggplot(aes(x=reorder(geography, stops), y=stops))+
  # Plot bars by geography in order of stops
  geom_bar(width = 0.85, stat = "identity", fill = "lightblue", color = "black")+
  # Define bars' parameters on bar plot
  scale_y_continuous(label=comma, expand = c(0, 0))+
  # Add comma thousand separators on y-axis labels and remove gap between axis and bars
  scale_x_discrete()+
  # Set x-axis as discrete
  labs(title="Total number of stop-and-searches per geographical\nregion in UK between 2006-2020",
       x="Location\n",
       # Newline character to create a gap between axis label and axis tick labels
       y="Number of stops")+
  theme(axis.title = element_text(size = 14),
        plot.title = element_text(size = 16),
        axis.line = element_line(size = 1),
        axis.text.y = element_text(size = 7),
        axis.ticks = element_line(size = 1),
        panel.background = element_blank(),
        panel.grid.major.x = element_line(color="lightgrey"))+
  # Resize title, axis labels, axis tick labels, axis ticks, remove panel background and customize grid
  coord_flip()
  # Flip x and y-axis

```
Clearly, the area governed by the Metropolitan Police (i.e. Greater London region, excluding the City of London) is where most stop-and-searches occur in UK.
This may be attributed to a higher population concentration, however, further analysis is required to establish that.



3)Are any of the ethnic categories related to stop-and-searches under one legal article section?

``` {r Q3}

stop_search_2 %>% filter(ethnicity != "All" & ethnicity != "Other") %>%
  # Filter out "All" and "Other" ethnicity groups
  ggplot(aes(x=legislation_type, y=stops, fill=ethnicity))+
  # Map values to plot
  geom_bar(position="fill", stat="identity")+
  # "fill" to stack bars
  scale_y_continuous(labels=percent_format(), expand = c(0,0))+
  # Change y-axis scale to continuous to enable percentage format. Also remove gap between axis and bars
  scale_x_discrete(expand = c(0,0))+
  # Set x-axis as discrete and remove gap between bars and axis
  coord_flip()+
  # Flip x and y-axis
  labs(title = "Percentage of stops under each legislation by ethnicity",
       x = "Legislation",
       y = "Stops",
       fill = "Ethnicity")
  # Change label text

```

Overall, people of Asian ethnicity seem to be mostly affected by Section 44/47a, with people of Black ethnicity being most affected by Section 60 and people of White
ethnicity being affect by All legislation types on average. However, it is worth noting that this data does not cover equally-sized samples of each ethnic category.
